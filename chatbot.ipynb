{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyCLMA5QUUBz0MFGN2Gkyd7DgCkZHTSOTu4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Access the secret\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "print(gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load documents correctly with LangChain's loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('./documents/', glob=\"**/*.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split documents properly\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize embeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=gemini_api_key,\n",
    "    model=\"models/text-embedding-004\"\n",
    ")\n",
    "\n",
    "# 4. Create fresh Chroma instance\n",
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs retrieved: 4\n",
      "First doc content: I was touched. I said something in condolence with him. I hinted that of course he did wisely in abstaining from writing for a while; and urged him to embrace that opportunity of taking wholesome exer\n"
     ]
    }
   ],
   "source": [
    "# 5. Test retrieval\n",
    "docs = vectorstore.similarity_search(\"bartleby\", k=4)\n",
    "print(f\"Number of docs retrieved: {len(docs)}\")\n",
    "if docs:\n",
    "    print(f\"First doc content: {docs[0].page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding function initialized: client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x78118846d3a0> model='models/text-embedding-004' task_type=None google_api_key=SecretStr('**********') credentials=None client_options=None transport=None request_options=None\n",
      "Chroma vectorstore initialized: <langchain_chroma.vectorstores.Chroma object at 0x7812d6372150>\n",
      "Retriever initialized: tags=['Chroma', 'GoogleGenerativeAIEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7812d6372150> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
    "# Initialize the embedding function with your Gemini API key\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(\n",
    "    google_api_key=os.environ[\n",
    "        \"GEMINI_API_KEY\"\n",
    "    ],  # Note: using google_api_key instead of api_key\n",
    "    model=\"models/text-embedding-004\",\n",
    ")\n",
    "\n",
    "# # Initialize the Chroma vector store\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"./chroma_data\", embedding_function=embedding_function\n",
    "# )\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "# Debug statements\n",
    "print(\"Embedding function initialized:\", embedding_function)\n",
    "print(\"Chroma vectorstore initialized:\", vectorstore)\n",
    "print(\"Retriever initialized:\", retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "# Initialize the language model with your Gemini API key\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key=os.environ[\"GEMINI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough  \n",
    "\n",
    "# Define your prompt template\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question based on the provided context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the StuffDocumentsChain with the prompt\n",
    "stuff_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Create the retrieval chain with the correct input key mapper\n",
    "qa_chain = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"input\"]),\n",
    "        \"question\": lambda x: x[\"input\"],\n",
    "    }\n",
    "    | stuff_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, the narrator admits that very little is known about Bartleby.  The narrator only knows what he witnessed personally, and a vague rumor (that Bartleby had been a subordinate clerk in the Dead Letter Office) heard after Bartleby's death, the truth of which is uncertain.  Beyond that,  Bartleby remains largely a mystery.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"who was bartleby?\"  # Simpler, direct question\n",
    "response = qa_chain.invoke({\"input\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administor/code_content/lang_chain_project/langchain_env/lib/python3.12/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "/home/administor/code_content/lang_chain_project/langchain_env/lib/python3.12/site-packages/gradio/chat_interface.py:229: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "def slow_echo(message, history):\n",
    "    question = message\n",
    "    response = qa_chain.invoke({\"input\": question})\n",
    "    for i in range(len(response)):\n",
    "        time.sleep(0.01)\n",
    "        yield \"Response:\" + response[: i+1]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸ“š Document Q&A Assistant\n",
    "    Ask questions about your documents and get AI-powered answers.\n",
    "    \"\"\")\n",
    "    \n",
    "    chatbot = gr.ChatInterface(\n",
    "        fn=slow_echo,\n",
    "        chatbot=gr.Chatbot(\n",
    "            elem_id=\"chatbot\",\n",
    "            bubble_full_width=True,\n",
    "            avatar_images=( \"icons/user_1.png\" , \"icons/bot_1.png\"),\n",
    "            height=500\n",
    "        ),\n",
    "        title=\"Document Q&A\",\n",
    "        description=\"Ask me anything about your documents\",\n",
    "        theme=\"soft\",\n",
    "        examples=[\n",
    "            \"What is the main theme of the document?\",\n",
    "            \"Can you summarize the key points?\",\n",
    "            \"Who are the main characters mentioned?\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
